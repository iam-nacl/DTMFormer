import os
import numpy as np
import torch
from skimage import io, color
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms as T
from torchvision.transforms import functional as F
from typing import Callable
import os
import cv2
import pandas as pd
from numbers import Number
from typing import Container
from collections import defaultdict
from einops import rearrange, repeat
import torchvision.transforms as transforms


def to_long_tensor(pic):
    # handle numpy array
    img = torch.from_numpy(np.array(pic, np.uint8))
    # backward compatibility
    return img.long()


def correct_dims(*images):
    corr_images = []
    # print(images)
    for img in images:
        if len(img.shape) == 2:
            corr_images.append(np.expand_dims(img, axis=2))
        else:
            corr_images.append(img)
    if len(corr_images) == 1:
        return corr_images[0]
    else:
        return corr_images


def norm_zscore(tx):
    tx = np.array(tx)
    tx = tx.astype(np.float32)
    tx_flat = tx.flatten()
    if np.sum(tx_flat) > 0:
        tx_flat_no = tx_flat[tx_flat > 0]
        tx_normal = (tx - np.mean(tx_flat_no)) / (np.std(tx_flat_no) + 1e-5)
        tx_normal[tx == 0] = 0
    else:
        tx_normal = tx
    return tx_normal


def make_multi_mask0(mask, num_classes):
    h, w = mask.shape
    mask3 = torch.zeros(h // 16, w // 16).type_as(mask) - 1
    mask2 = torch.zeros(h // 4, w // 4).type_as(mask) - 1
    mask1 = torch.zeros(h, w).type_as(mask) + mask
    # ----------generate the deepest mask-------------
    mask_l3 = rearrange(mask, '(h b) (w d) -> h w (b d)', b=16, d=16)
    mask_l2 = rearrange(mask, '(h b) (w d) -> h w (b d)', b=4, d=4)
    for i in range(num_classes):
        flagi3 = torch.abs(mask_l3 - i)
        flagi3 = torch.sum(flagi3, dim=-1)
        mask3[flagi3 == 0] = i
        flagi2 = torch.abs(mask_l2 - i)
        flagi2 = torch.sum(flagi2, dim=-1)
        mask2[flagi2 == 0] = i
    gt32 = mask3[:, :, None].repeat(1, 1, 16)
    gt32 = rearrange(gt32, 'h w (c d) -> (h c) (w d)', c=4, d=4)
    gt31 = gt32[:, :, None].repeat(1, 1, 16)
    gt31 = rearrange(gt31, 'h w (c d) -> (h c) (w d)', c=4, d=4)
    # --------generate the second deepest mask--------------
    mask2[gt32 == mask2] = -1
    gt21 = mask2[:, :, None].repeat(1, 1, 16)
    gt21 = rearrange(gt21, 'h w (c d) -> (h c) (w d)', c=4, d=4)
    # -------------generate the first mask------------------
    mask1[gt31 == mask] = -1
    mask1[gt21 == mask] = -1
    return mask, mask1, mask2, mask3

def make_multi_mask(mask, num_classes):
    h, w = mask.shape
    mask3 = torch.zeros(h // 4, w // 4).type_as(mask) - 1
    mask2 = torch.zeros(h // 2, w // 2).type_as(mask) - 1
    mask1 = torch.zeros(h, w).type_as(mask) + mask
    # ----------generate the deepest mask-------------
    mask_l3 = rearrange(mask, '(h b) (w d) -> h w (b d)', b=4, d=4)
    mask_l2 = rearrange(mask, '(h b) (w d) -> h w (b d)', b=2, d=2)
    for i in range(num_classes):
        flagi3 = torch.abs(mask_l3 - i)
        flagi3 = torch.sum(flagi3, dim=-1)
        mask3[flagi3 == 0] = i
        flagi2 = torch.abs(mask_l2 - i)
        flagi2 = torch.sum(flagi2, dim=-1)
        mask2[flagi2 == 0] = i
    return mask, mask1, mask2, mask3

def make_multi_mask_possibility(mask, num_classes):
    h, w = mask.shape
    mask3 = torch.zeros(num_classes, h // 4, w // 4).float()
    mask2 = torch.zeros(num_classes, h // 2, w // 2).float()
    mask1 = torch.zeros(num_classes, h, w).float()
    # ----------generate the deepest mask-------------
    mask_l3 = rearrange(mask, '(h b) (w d) -> h w (b d)', b=4, d=4)
    mask_l2 = rearrange(mask, '(h b) (w d) -> h w (b d)', b=2, d=2)
    for i in range(num_classes):
        flagi3 = torch.abs(mask_l3 - i)  # h w np
        flagi3[flagi3 != 0] = 1
        flagi3 = (flagi3 + 1) % 2
        flagi3 = torch.sum(flagi3, dim=-1)  # h w
        mask3[i, :, :] = flagi3/(4*4)

        flagi2 = torch.abs(mask_l2 - i)
        flagi2[flagi2 != 0] = 1
        flagi2 = (flagi2 + 1) % 2
        flagi2 = torch.sum(flagi2, dim=-1)
        mask2[i, :, :] = flagi2/(2*2)

        flagi1 = torch.abs(mask - i)
        flagi1[flagi1 != 0] = 1
        flagi1 = (flagi1 + 1) % 2
        mask1[i, :, :] = flagi1
    return mask, mask1, mask2, mask3

class JointTransform2D:
    """
    Performs augmentation on image and mask when called. Due to the randomness of augmentation transforms,
    it is not enough to simply apply the same Transform from torchvision on the image and mask separetely.
    Doing this will result in messing up the ground truth mask. To circumvent this problem, this class can
    be used, which will take care of the problems above.

    Args:
        crop: tuple describing the size of the random crop. If bool(crop) evaluates to False, no crop will
            be taken.
        p_flip: float, the probability of performing a random horizontal flip.
        color_jitter_params: tuple describing the parameters of torchvision.transforms.ColorJitter.
            If bool(color_jitter_params) evaluates to false, no color jitter transformation will be used.
        p_random_affine: float, the probability of performing a random affine transform using
            torchvision.transforms.RandomAffine.
        long_mask: bool, if True, returns the mask as LongTensor in label-encoded format.
    """

    def __init__(self, img_size=256, crop=(32, 32), p_flip=0.0, p_rota=0.0, p_scale=0.0, p_gaussn=0.0, p_contr=0.0,
                 p_gama=0.0, p_distor=0.0, z_score=False, color_jitter_params=(0.1, 0.1, 0.1, 0.1), p_random_affine=0,
                 long_mask=False):
        self.crop = crop
        self.p_flip = p_flip
        self.p_rota = p_rota
        self.p_scale = p_scale
        self.p_gaussn = p_gaussn
        self.p_gama = p_gama
        self.p_contr = p_contr
        self.p_distortion = p_distor
        self.zscore = z_score
        self.img_size = img_size
        self.color_jitter_params = color_jitter_params
        if color_jitter_params:
            self.color_tf = T.ColorJitter(*color_jitter_params)
        self.p_random_affine = p_random_affine
        self.long_mask = long_mask

    def __call__(self, image, mask):
        #  gamma enhancement
        if np.random.rand() < self.p_gama:
            c = 1
            g = np.random.randint(10, 25) / 10.0
            # g = 2
            image = (np.power(image / 255, 1.0 / g) / c) * 255
            image = image.astype(np.uint8)
        # transforming to PIL image
        image, mask = F.to_pil_image(image), F.to_pil_image(mask)
        # random crop
        if self.crop:
            i, j, h, w = T.RandomCrop.get_params(image, self.crop)
            image, mask = F.crop(image, i, j, h, w), F.crop(mask, i, j, h, w)
        # random horizontal flip
        if np.random.rand() < self.p_flip:
            image, mask = F.hflip(image), F.hflip(mask)
        # random rotation
        if np.random.rand() < self.p_rota:
            angle = T.RandomRotation.get_params((-30, 30))
            image, mask = F.rotate(image, angle), F.rotate(mask, angle)
        # random scale and center resize to the original size
        if np.random.rand() < self.p_scale:
            scale = np.random.uniform(1, 1.3)
            new_h, new_w = int(self.img_size * scale), int(self.img_size * scale)
            image, mask = F.resize(image, (new_h, new_w), 2), F.resize(mask, (new_h, new_w), 0)
            # image = F.center_crop(image, (self.img_size, self.img_size))
            # mask = F.center_crop(mask, (self.img_size, self.img_size))
            i, j, h, w = T.RandomCrop.get_params(image, (self.img_size, self.img_size))
            image, mask = F.crop(image, i, j, h, w), F.crop(mask, i, j, h, w)
        # random add gaussian noise
        if np.random.rand() < self.p_gaussn:
            ns = np.random.randint(3, 15)
            noise = np.random.normal(loc=0, scale=1, size=(self.img_size, self.img_size)) * ns
            noise = noise.astype(int)
            image = np.array(image) + noise
            image[image > 255] = 255
            image[image < 0] = 0
            image = F.to_pil_image(image.astype('uint8'))
        # random change the contrast
        if np.random.rand() < self.p_contr:
            contr_tf = T.ColorJitter(contrast=(0.8, 2.0))
            image = contr_tf(image)
        # random distortion
        if np.random.rand() < self.p_distortion:
            distortion = T.RandomAffine(0, None, None, (5, 30))
            image = distortion(image)
        # color transforms || ONLY ON IMAGE
        if self.color_jitter_params:
            image = self.color_tf(image)
        # random affine transform
        if np.random.rand() < self.p_random_affine:
            affine_params = T.RandomAffine(180).get_params((-90, 90), (1, 1), (2, 2), (-45, 45), self.crop)
            image, mask = F.affine(image, *affine_params), F.affine(mask, *affine_params)
        # transforming to tensor
        if self.zscore:
            image = norm_zscore(image)
            image = torch.from_numpy(image[None, :, :])
        else:
            image = F.to_tensor(image)

        if not self.long_mask:
            mask = F.to_tensor(mask)
        else:
            mask = to_long_tensor(mask)
        return image, mask


class ImageToImage2D(Dataset):
    """
    Reads the images and applies the augmentation transform on them.
    Usage:
        1. If used without the unet.model.Model wrapper, an instance of this object should be passed to
           torch.utils.data.DataLoader. Iterating through this returns the tuple of image, mask and image
           filename.
        2. With unet.model.Model wrapper, an instance of this object should be passed as train or validation
           datasets.

    Args:
        dataset_path: path to the dataset. Structure of the dataset should be:
            dataset_path
              |-- images
                  |-- img001.png
                  |-- img002.png
                  |-- ...
              |-- masks
                  |-- img001.png
                  |-- img002.png
                  |-- ...

        joint_transform: augmentation transform, an instance of JointTransform2D. If bool(joint_transform)
            evaluates to False, torchvision.transforms.ToTensor will be used on both image and mask.
        one_hot_mask: bool, if True, returns the mask in one-hot encoded form.
    """

    def __init__(self, dataset_path: str, split='train1', joint_transform: Callable = None, classes=2,
                 one_hot_mask: int = False) -> None:
        self.dataset_path = dataset_path
        self.img_path = os.path.join(dataset_path, 'img')
        self.label_path = os.path.join(dataset_path, 'label')
        self.classes = classes
        self.one_hot_mask = one_hot_mask
        id_list_file = os.path.join(dataset_path, 'MainPatient/{0}.txt'.format(split))
        self.ids = [id_.strip() for id_ in open(id_list_file)]
        self.to_norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        if joint_transform:
            self.joint_transform = joint_transform
        else:
            to_tensor = T.ToTensor()
            self.joint_transform = lambda x, y: (to_tensor(x), to_tensor(y))

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, i):
        id_ = self.ids[i]
        image = cv2.imread(os.path.join(self.img_path, id_ + '.png'))
        mask = cv2.imread(os.path.join(self.label_path, id_ + '.png'), 0)
        # correct dimensions if needed
        if self.classes == 2:
            mask[mask >= 1] = 1
            mask[mask < 1] = 0
        image, mask = correct_dims(image, mask)
        if self.joint_transform:
            image, mask = self.joint_transform(image, mask)
        #image = self.to_norm(image)
        if self.one_hot_mask:
            assert self.one_hot_mask > 0, 'one_hot_mask must be nonnegative'
            mask = torch.zeros((self.one_hot_mask, mask.shape[1], mask.shape[2])).scatter_(0, mask.long(), 1)
        mask, mask1, mask2, mask3 = make_multi_mask_possibility(mask, self.classes)
        return image, mask, mask1, mask2, mask3, id_ + '.png'
        #return image, mask, id_ + '.png'

class ImageToImage2D_Substation(Dataset):
    """
    Reads the images and applies the augmentation transform on them.
    Usage:
        1. If used without the unet.model.Model wrapper, an instance of this object should be passed to
           torch.utils.data.DataLoader. Iterating through this returns the tuple of image, mask and image
           filename.
        2. With unet.model.Model wrapper, an instance of this object should be passed as train or validation
           datasets.

    Args:
        dataset_path: path to the dataset. Structure of the dataset should be:
            dataset_path
              |-- images
                  |-- img001.png
                  |-- img002.png
                  |-- ...
              |-- masks
                  |-- img001.png
                  |-- img002.png
                  |-- ...

        joint_transform: augmentation transform, an instance of JointTransform2D. If bool(joint_transform)
            evaluates to False, torchvision.transforms.ToTensor will be used on both image and mask.
        one_hot_mask: bool, if True, returns the mask in one-hot encoded form.
    """

    def __init__(self, dataset_path: str, split='train1', joint_transform: Callable = None, classes=2,
                 one_hot_mask: int = False) -> None:
        self.dataset_path = dataset_path
        self.img_path = os.path.join(dataset_path, 'images')
        self.label_path = os.path.join(dataset_path, '15_masks')
        self.classes = classes
        self.one_hot_mask = one_hot_mask
        id_list_file = os.path.join(dataset_path, 'MainPatient/{0}.txt'.format(split))
        # self.ids = [id_.strip() for id_ in open(id_list_file)]
        self.ids = [id_.strip().split('/')[2] for id_ in open(id_list_file)]
        self.to_norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        if joint_transform:
            self.joint_transform = joint_transform
        else:
            to_tensor = T.ToTensor()
            self.joint_transform = lambda x, y: (to_tensor(x), to_tensor(y))

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, i):
        id_ = self.ids[i]
        # image = cv2.imread(os.path.join(self.img_path, id_ + '.png'), 0)
        filename1 = os.path.join(self.img_path, id_ + '.JPG')
        filename2 = os.path.join(self.img_path, id_ + '.jpg')
        filename3 = os.path.join(self.img_path, id_ + '.jpeg')  # 或者 .jpeg
        filename4 = os.path.join(self.img_path, id_ + '.JPEG')  # 或者 .jpeg
        if os.path.exists(filename1):
            image = cv2.imread(filename1)
        elif os.path.exists(filename2):
            image = cv2.imread(filename2)
        elif os.path.exists(filename3):
            image = cv2.imread(filename3)
        elif os.path.exists(filename4):
            image = cv2.imread(filename4)
        else:
            print(filename1)
            pass
        mask = cv2.imread(os.path.join(self.label_path, id_ + '.png'), 0)
        # correct dimensions if needed
        if self.classes == 2:
            mask[mask >= 1] = 1
            mask[mask < 1] = 0
        image, mask = correct_dims(image, mask)
        if self.joint_transform:
            image, mask = self.joint_transform(image, mask)
        #image = self.to_norm(image)
        if self.one_hot_mask:
            assert self.one_hot_mask > 0, 'one_hot_mask must be nonnegative'
            mask = torch.zeros((self.one_hot_mask, mask.shape[1], mask.shape[2])).scatter_(0, mask.long(), 1)
        mask, mask1, mask2, mask3 = make_multi_mask_possibility(mask, self.classes)
        return image, mask, mask1, mask2, mask3, id_ + '.png'
        #return image, mask, id_ + '.png'

class ImageToImage2D_Substation2(Dataset):
    """
    Reads the images and applies the augmentation transform on them.
    Usage:
        1. If used without the unet.model.Model wrapper, an instance of this object should be passed to
           torch.utils.data.DataLoader. Iterating through this returns the tuple of image, mask and image
           filename.
        2. With unet.model.Model wrapper, an instance of this object should be passed as train or validation
           datasets.

    Args:
        dataset_path: path to the dataset. Structure of the dataset should be:
            dataset_path
              |-- images
                  |-- img001.png
                  |-- img002.png
                  |-- ...
              |-- masks
                  |-- img001.png
                  |-- img002.png
                  |-- ...

        joint_transform: augmentation transform, an instance of JointTransform2D. If bool(joint_transform)
            evaluates to False, torchvision.transforms.ToTensor will be used on both image and mask.
        one_hot_mask: bool, if True, returns the mask in one-hot encoded form.
    """

    def __init__(self, dataset_path: str, split='train1', joint_transform: Callable = None, classes=2,
                 one_hot_mask: int = False) -> None:
        self.dataset_path = dataset_path
        self.img_path = os.path.join(dataset_path, 'img256')
        self.label_path = os.path.join(dataset_path, 'label256')
        self.classes = classes
        self.one_hot_mask = one_hot_mask
        id_list_file = os.path.join(dataset_path, 'MainPatient/{0}.txt'.format(split))
        # self.ids = [id_.strip() for id_ in open(id_list_file)]
        self.ids = [id_.strip().split('/')[2] for id_ in open(id_list_file)]
        self.to_norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        if joint_transform:
            self.joint_transform = joint_transform
        else:
            to_tensor = T.ToTensor()
            self.joint_transform = lambda x, y: (to_tensor(x), to_tensor(y))

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, i):
        id_ = self.ids[i]
        image = cv2.imread(os.path.join(self.img_path, id_ + '.png'))
        mask = cv2.imread(os.path.join(self.label_path, id_ + '.png'), 0)
        # correct dimensions if needed
        if self.classes == 2:
            mask[mask >= 1] = 1
            mask[mask < 1] = 0
        image, mask = correct_dims(image, mask)
        if self.joint_transform:
            image, mask = self.joint_transform(image, mask)
        #image = self.to_norm(image)
        if self.one_hot_mask:
            assert self.one_hot_mask > 0, 'one_hot_mask must be nonnegative'
            mask = torch.zeros((self.one_hot_mask, mask.shape[1], mask.shape[2])).scatter_(0, mask.long(), 1)
        mask, mask1, mask2, mask3 = make_multi_mask_possibility(mask, self.classes)
        return image, mask, mask1, mask2, mask3, id_ + '.png'
        #return image, mask, id_ + '.png'


class Image2D(Dataset):
    """
    Reads the images and applies the augmentation transform on them. As opposed to ImageToImage2D, this
    reads a single image and requires a simple augmentation transform.
    Usage:
        1. If used without the unet.model.Model wrapper, an instance of this object should be passed to
           torch.utils.data.DataLoader. Iterating through this returns the tuple of image and image
           filename.
        2. With unet.model.Model wrapper, an instance of this object should be passed as a prediction
           dataset.

    Args:

        dataset_path: path to the dataset. Structure of the dataset should be:
            dataset_path
              |-- images
                  |-- img001.png
                  |-- img002.png
                  |-- ...

        transform: augmentation transform. If bool(joint_transform) evaluates to False,
            torchvision.transforms.ToTensor will be used.
    """

    def __init__(self, dataset_path: str, split='val', transform: Callable = None):
        self.dataset_path = dataset_path
        self.img_path = os.path.join(dataset_path, 'valimgcrop')
        #self.label_path = os.path.join(dataset_path, 'label')
        id_list_file = os.path.join(dataset_path, 'MainPatient/{0}.txt'.format(split))
        self.ids = [id_.strip() for id_ in open(id_list_file)]
        #self.to_norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        if transform:
            self.transform = transform
        else:
            self.transform = T.ToTensor()

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, i):
        id_ = self.ids[i]
        image = cv2.imread(os.path.join(self.img_path, id_ + '.png'))
        image = correct_dims(image)
        image = self.transform(image)
        #image = self.to_norm(image)
        return image, id_ + '.png'
